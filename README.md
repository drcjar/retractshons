so herein we have a ipython notebook that we use to get a list of urls to download

we use wget to download them using the .scr file which is from a bash script I found on the internet

every 400 pages or so google blocks us and challenges us to prove we're not a robot

we delete our existing cookies, prove we're not a robot, obtain a new cookie and save it to the working directory using a 'save cookie' plugin for firefox

when we have the pages we process them using our ipython notebook

@thatdavidmiller wrote the functions in the ipython notebook. I wrote the pandas codes and wonky regex
